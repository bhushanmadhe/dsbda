{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1d6c0e-e4e7-45f0-a9f3-07629b899e21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ab8c79-d305-40cf-bdf9-3b47ed453bcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4154ba9-096e-41cf-a020-e86367a393eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1389adf5-102c-4261-86cb-d5dd27d16e41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59075b96-7741-4e8e-a36e-b917b00b6a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Document:\n",
      "Natural Language Processing helps computers understand human language.\n",
      "\n",
      "Tokenized:\n",
      "['Natural', 'Language', 'Processing', 'helps', 'computers', 'understand', 'human', 'language', '.']\n",
      "\n",
      "POS Tags:\n",
      "[('Natural', 'JJ'), ('Language', 'NNP'), ('Processing', 'NNP'), ('helps', 'VBZ'), ('computers', 'NNS'), ('understand', 'VBP'), ('human', 'JJ'), ('language', 'NN'), ('.', '.')]\n",
      "\n",
      "Filtered (Stopwords Removed):\n",
      "['Natural', 'Language', 'Processing', 'helps', 'computers', 'understand', 'human', 'language', '.']\n",
      "\n",
      "Stemmed Tokens:\n",
      "['natur', 'languag', 'process', 'help', 'comput', 'understand', 'human', 'languag', '.']\n",
      "\n",
      "Lemmatized Tokens:\n",
      "['Natural', 'Language', 'Processing', 'help', 'computer', 'understand', 'human', 'language', '.']\n",
      "\n",
      "Term Frequency (TF) for each document:\n",
      "\n",
      "Document 1:\n",
      "Natural: 0.1111\n",
      "Language: 0.1111\n",
      "Processing: 0.1111\n",
      "helps: 0.1111\n",
      "computers: 0.1111\n",
      "understand: 0.1111\n",
      "human: 0.1111\n",
      "language: 0.1111\n",
      ".: 0.1111\n",
      "\n",
      "Document 2:\n",
      "Document: 0.0909\n",
      "processing: 0.0909\n",
      "includes: 0.0909\n",
      "various: 0.0909\n",
      "tasks: 0.0909\n",
      "such: 0.0909\n",
      "as: 0.0909\n",
      "tokenization: 0.0909\n",
      "and: 0.0909\n",
      "lemmatization: 0.0909\n",
      ".: 0.0909\n",
      "\n",
      "Document 3:\n",
      "Stemming: 0.1111\n",
      "is: 0.1111\n",
      "a: 0.1111\n",
      "technique: 0.1111\n",
      "used: 0.1111\n",
      "in: 0.1111\n",
      "text: 0.1111\n",
      "preprocessing: 0.1111\n",
      ".: 0.1111\n",
      "\n",
      "Inverse Document Frequency (IDF):\n",
      "Processing: 0.4055\n",
      "Language: 0.4055\n",
      "helps: 0.4055\n",
      "computers: 0.4055\n",
      "language: 0.4055\n",
      ".: -0.2877\n",
      "Natural: 0.4055\n",
      "human: 0.4055\n",
      "understand: 0.4055\n",
      "processing: 0.4055\n",
      "such: 0.4055\n",
      "as: 0.4055\n",
      "lemmatization: 0.4055\n",
      "tasks: 0.4055\n",
      "and: 0.4055\n",
      "includes: 0.4055\n",
      "tokenization: 0.4055\n",
      "various: 0.4055\n",
      "Document: 0.4055\n",
      "used: 0.4055\n",
      "Stemming: 0.4055\n",
      "a: 0.4055\n",
      "text: 0.4055\n",
      "preprocessing: 0.4055\n",
      "in: 0.4055\n",
      "is: 0.4055\n",
      "technique: 0.4055\n",
      "\n",
      "TF-IDF for each document:\n",
      "\n",
      "Document 1:\n",
      "Natural: 0.0451\n",
      "Language: 0.0451\n",
      "Processing: 0.0451\n",
      "helps: 0.0451\n",
      "computers: 0.0451\n",
      "understand: 0.0451\n",
      "human: 0.0451\n",
      "language: 0.0451\n",
      ".: -0.0320\n",
      "\n",
      "Document 2:\n",
      "Document: 0.0369\n",
      "processing: 0.0369\n",
      "includes: 0.0369\n",
      "various: 0.0369\n",
      "tasks: 0.0369\n",
      "such: 0.0369\n",
      "as: 0.0369\n",
      "tokenization: 0.0369\n",
      "and: 0.0369\n",
      "lemmatization: 0.0369\n",
      ".: -0.0262\n",
      "\n",
      "Document 3:\n",
      "Stemming: 0.0451\n",
      "is: 0.0451\n",
      "a: 0.0451\n",
      "technique: 0.0451\n",
      "used: 0.0451\n",
      "in: 0.0451\n",
      "text: 0.0451\n",
      "preprocessing: 0.0451\n",
      ".: -0.0320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user5\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user5\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user5\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\user5\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from math import log\n",
    "import pandas as pd\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Sample document\n",
    "documents = [\n",
    "    \"Natural Language Processing helps computers understand human language.\",\n",
    "    \"Document processing includes various tasks such as tokenization and lemmatization.\",\n",
    "    \"Stemming is a technique used in text preprocessing.\"\n",
    "]\n",
    "\n",
    "# Preprocessing Functions\n",
    "def preprocess_document(text):\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # POS Tagging\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    \n",
    "    # Stop Words Removal\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    filtered = [w for w in tokens if w.lower() not in stop_words]\n",
    "    \n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed = [stemmer.stem(w) for w in filtered]\n",
    "    \n",
    "    # Lemmatization with POS tagging\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Function to map POS tag to WordNet POS\n",
    "    def get_wordnet_pos(treebank_tag):\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return 'a'  # Adjective\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return 'v'  # Verb\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return 'n'  # Noun\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return 'r'  # Adverb\n",
    "        else:\n",
    "            return 'n'  # Default to noun\n",
    "\n",
    "    lemmatized = [\n",
    "        lemmatizer.lemmatize(word, pos=get_wordnet_pos(tag)) for word, tag in pos_tags\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        \"tokens\": tokens,\n",
    "        \"pos_tags\": pos_tags,\n",
    "        \"filtered_tokens\": filtered,\n",
    "        \"stemmed_tokens\": stemmed,\n",
    "        \"lemmatized_tokens\": lemmatized\n",
    "    }\n",
    "\n",
    "# Process each document\n",
    "processed_documents = [preprocess_document(doc) for doc in documents]\n",
    "\n",
    "# Manually calculate Term Frequency (TF)\n",
    "def calculate_tf(document):\n",
    "    tokens = word_tokenize(document)\n",
    "    token_count = len(tokens)\n",
    "    \n",
    "    word_counts = {}\n",
    "    for word in tokens:\n",
    "        word_counts[word] = word_counts.get(word, 0) + 1\n",
    "\n",
    "    # Calculate TF for each word\n",
    "    tf_values = {word: count / token_count for word, count in word_counts.items()}\n",
    "    return tf_values\n",
    "\n",
    "# Calculate Inverse Document Frequency (IDF)\n",
    "def calculate_idf(documents):\n",
    "    N = len(documents)  # Total number of documents\n",
    "    word_doc_count = {}\n",
    "    \n",
    "    # Count how many documents contain each word\n",
    "    for doc in documents:\n",
    "        words_in_doc = set(word_tokenize(doc))  # Unique words in each document\n",
    "        for word in words_in_doc:\n",
    "            word_doc_count[word] = word_doc_count.get(word, 0) + 1\n",
    "\n",
    "    # Calculate IDF for each word\n",
    "    idf_values = {}\n",
    "    for word, count in word_doc_count.items():\n",
    "        idf_values[word] = log(N / (1 + count))  # +1 to avoid division by zero\n",
    "\n",
    "    return idf_values\n",
    "\n",
    "# Output Preprocessed Results for the first document\n",
    "print(\"Original Document:\")\n",
    "print(documents[0])\n",
    "\n",
    "print(\"\\nTokenized:\")\n",
    "print(processed_documents[0][\"tokens\"])\n",
    "\n",
    "print(\"\\nPOS Tags:\")\n",
    "print(processed_documents[0][\"pos_tags\"])\n",
    "\n",
    "print(\"\\nFiltered (Stopwords Removed):\")\n",
    "print(processed_documents[0][\"filtered_tokens\"])\n",
    "\n",
    "print(\"\\nStemmed Tokens:\")\n",
    "print(processed_documents[0][\"stemmed_tokens\"])\n",
    "\n",
    "print(\"\\nLemmatized Tokens:\")\n",
    "print(processed_documents[0][\"lemmatized_tokens\"])\n",
    "\n",
    "# Calculate Term Frequency (TF) for all documents\n",
    "print(\"\\nTerm Frequency (TF) for each document:\")\n",
    "for i, doc in enumerate(documents):\n",
    "    tf_values = calculate_tf(doc)\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    for word, tf in tf_values.items():\n",
    "        print(f\"{word}: {tf:.4f}\")\n",
    "\n",
    "# Calculate Inverse Document Frequency (IDF)\n",
    "idf_values = calculate_idf(documents)\n",
    "print(\"\\nInverse Document Frequency (IDF):\")\n",
    "for word, idf in idf_values.items():\n",
    "    print(f\"{word}: {idf:.4f}\")\n",
    "\n",
    "# Calculate the final TF-IDF values for each document\n",
    "print(\"\\nTF-IDF for each document:\")\n",
    "for i, doc in enumerate(documents):\n",
    "    tf_values = calculate_tf(doc)\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    for word in tf_values:\n",
    "        tfidf_value = tf_values[word] * idf_values.get(word, 0)\n",
    "        print(f\"{word}: {tfidf_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fec55a-0510-4873-b060-dafb31a6601e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
